---
title: "Assignment_5"
author: "Cole Yagersz"
output: html_document
---
## Introduction

The dataset Cereals.csv includes nutritional information, store display, and consumer ratings for 77 breakfast cereals.

Data Preprocessing. Remove all cereals with missing values.

● Apply hierarchical clustering to the data using Euclidean distance to the normalized
measurements. Use Agnes to compare the clustering from single linkage, complete
linkage, average linkage, and Ward. Choose the best method.

● How many clusters would you choose?

● Comment on the structure of the clusters and on their stability. Hint: To check stability,
partition the data and see how well clusters formed based on one part apply to the other
part. To do this:

● Cluster partition A

● Use the cluster centroids from A to assign each record in partition B (each record
is assigned to the cluster with the closest centroid).

● Assess how consistent the cluster assignments are compared to the
assignments based on all the data.

● The elementary public schools would like to choose a set of cereals to include in their
daily cafeterias. Every day a different cereal is offered, but all cereals should support a
healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.”
Should the data be normalized? If not, how should they be used in the cluster analysis?
Load Libraries
```{r}
library(caret)
library(factoextra)
library(ggplot2)
library(cluster)
```
Question 1:

Apply hierarchical clustering to the data using Euclidean distance to the normalized
measurements. Use Agnes to compare the clustering from single linkage, complete
linkage, average linkage, and Ward. Choose the best method

Read the data and cleaned Missing values.
```{r}
# Read the dataset
cereal <- read.csv("Cereals.csv")

# set row names to the utilities column
row.names(cereal) <- cereal[,1]

# remove the utility column
cereal <- cereal[,-1]


cereal_clean <- na.omit(cereal)

head(cereal_clean)

```
Removed Categorical values so we can apply Euclidean Distance
```{r}
# Columns to drop 
cols_to_remove <- c("mfr", "type", "shelf")

# Drop categorical variables
cereal_numeric <- cereal_clean[, !(names(cereal_clean) %in% cols_to_remove)]

summary(cereal_numeric)
```

Normalized the data
```{r}
cereal_numeric_norm <- as.data.frame(scale(cereal_numeric))
row.names(cereal_numeric_norm) <- row.names(cereal_numeric)
head(cereal_numeric_norm)

```

Used Euclidean on the normalized data 
```{r}
cereal_dist <- dist(cereal_numeric_norm,method = "euclidean")
summary(cereal_dist)

```

Used hclust to compute and show the the single, complete, average, and ward linkages.
```{r}
hc1 <- hclust(cereal_dist, method = "single")
plot(hc1, hang = -1, ann = FALSE)

hc2 <- hclust(cereal_dist, method = "complete")
plot(hc2, hang = -1, ann = FALSE)

hc3 <- hclust(cereal_dist, method = "average")
plot(hc3, hang = -1, ann = FALSE)

hc4 <- hclust(cereal_dist, method = "ward.D")
plot(hc4, hang = -1, ann = FALSE)
```

Used Agnes and found the best method to be ward based on the Agglomerative coefficient
```{r}
agnes_single <- agnes(cereal_numeric_norm, method = "single")
print(agnes_single$ac)

agnes_complete <- agnes(cereal_numeric_norm, method = "complete")
print(agnes_complete$ac)

agnes_average <- agnes(cereal_numeric_norm, method = "average")
print(agnes_average$ac)


agnes_ward <- agnes(cereal_numeric_norm, method = "ward")
print(agnes_ward$ac)

```
Based on the results we got from comparing the different methods I found that the Ward method gives the best Agglomerative coefficient which is .9087.

Question 2:

How many clusters would you choose?

```{r}

plot(hc4, 
     hang = -1, 
     main = "Ward's Hierarchical Clustering Dendrogram",
     xlab = "Cereals",
     ylab = "Height",
     cex = 0.5)
rect.hclust(hc4,k=6,border=1:6)

plot(hc4, 
     hang = -1, 
     main = "Ward's Hierarchical Clustering Dendrogram",
     xlab = "Cereals",
     ylab = "Height",
     cex = 0.5)
rect.hclust(hc4,k=5,border=1:5)

```

I selected 6 clusters because the Ward dendrogram shows a jump in height when going from 6 to 5 clusters, showing that 6 clusters keeps the structure of the data.
```{r}
# Cut into k clusters
clusters <- cutree(hc4, k = 6)
table(clusters)

```

Added the cluster number to the data frame.
```{r}
cereal_clusters <- data.frame(cereal_numeric_norm, Cluster = clusters)
head(cereal_clusters)


```


Question 3:

Comment on the structure of the clusters and on their stability. Hint: To check stability,
partition the data and see how well clusters formed based on one part apply to the other
part. To do this:

● Cluster partition A

● Use the cluster centroids from A to assign each record in partition B (each record
is assigned to the cluster with the closest centroid).

● Assess how consistent the cluster assignments are compared to the
assignments based on all the data.

Partition the data 
```{r}
set.seed(123)
train_index <- createDataPartition(y=cereal_clusters[,1],p=.5)[[1]]
partition_A <- cereal_numeric_norm[train_index, ]
partition_B <- cereal_numeric_norm[-train_index, ]
```

```{r}
# Cluster Partition A
hcA <- hclust(dist(partition_A), method = "ward.D2")
clusters_A <- cutree(hcA, k = 6)

centroids_A <- aggregate(partition_A, by = list(cluster = clusters_A),FUN = mean)


centroids_A <- centroids_A[, -1]


assign_to_nearest <- function(x, centers) {
  x <- as.numeric(x)
  centers <- as.matrix(centers)

  dists <- apply(centers, 1, function(row_center) sum((x - row_center)^2))
  which.min(dists)
}

assigned_B <- apply(partition_B, 1, assign_to_nearest, centers = centroids_A)

assigned_B

# Cluster full data
hc_full <- hclust(dist(cereal_numeric_norm), method = "ward.D2")
clusters_full <- cutree(hc_full, k = 6)

# Compare stability
table(assigned_B, clusters_full[-train_index])

```
The stability table shows that most clusters remain consistent when assigning Partition B cereals based on Partition A centroids. Some cereals in cluster 2 shift between clusters, but overall, the clustering structure has medium to high stability.

Question 4:

The elementary public schools would like to choose a set of cereals to include in their
daily cafeterias. Every day a different cereal is offered, but all cereals should support a
healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.”
Should the data be normalized? If not, how should they be used in the cluster analysis?

```{r}

# Compute cluster summaries for the numeric variables
cluster_summary <- aggregate(cereal_numeric, by = list(Cluster = cereal_clusters$Cluster), FUN = mean)
cluster_summary

```
Based off the summary of the clusters I would say that the healthiest cereals are in clusters 1 and 5 having the lowest calories and sugars and having higher protein and fiber. I believe that the data should still be normalized because if you want to compare all variables to each other than you want them to be in the same units. After being normalized you are able to cluster these variables together. This allows for all the data to be used and compared to each other when trying to decide if a cereal is healthy or not.