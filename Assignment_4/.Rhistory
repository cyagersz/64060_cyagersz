# Separate predictors and labels for train and validation
train_X <- train_data[, !(names(train_data) %in% c("Personal.Loan"))]
train_Y <- train_data$Personal.Loan
valid_X <- valid_data[, !(names(valid_data) %in% c("Personal.Loan"))]
valid_Y <- valid_data$Personal.Loan
# Create new customer
new_customer <- data.frame(
Age = 40,
Experience = 10,
Income = 84,
Family = 2,
CCAvg = 2,
Mortgage = 0,
Securities.Account = 0,
CD.Account = 0,
Online = 1,
Credit.Card = 1,
Education1 = 0,
Education2 = 1,
Education3 = 0
)
new_customer[, c("Age", "Experience", "Income", "CCAvg", "Mortgage","Family")] <-
predict(norm_model, new_customer[, c("Age", "Experience", "Income", "CCAvg", "Mortgage","Family")])
# Run k-nn with k=1
prediction <- knn(train = train_X, test = new_customer, cl = train_Y, k = 1)
# Question 1 answer is the customer will not accept the loan
print(paste("The customer classification is", prediction))
library("caret")
library("FNN")
library("class")
accuracy.df <- data.frame(k = seq(1, 20, 1), Accuracy = rep(0, 20))
train_Y <- factor(train_Y)
valid_Y <- factor(valid_Y)
# compute knn for different k on validation.
for(i in 1:20) {
knn.pred <- knn(train_X, valid_X,
cl = train_Y, k = i)
accuracy.df[i, 2] <- confusionMatrix(knn.pred, valid_Y)$overall["Accuracy"]
}
accuracy.df
library("gmodels")
best_k <- 3
knn.best <- knn(train = train_X, test = valid_X, cl = train_Y, k = best_k)
CrossTable(x = valid_Y, y = knn.best, prop.chisq = FALSE)
prediction2 <- knn(train = train_X, test = new_customer, cl = train_Y, k = best_k)
print(paste("The customer classification is", prediction2))
set.seed(123)
test_index <- createDataPartition(bank$Personal.Loan, p = 0.2, list = FALSE) # 20%
test_data <- bank[test_index, ]
bank_data <- bank[-test_index, ]
train_index <- createDataPartition(bank_data$Personal.Loan, p = .5, list = FALSE) # 50%
train_data <- bank_data[train_index]
valid_data <- bank_data[-train_index] # 30%
summary(test_data)
summary(train_data)
summary(valid_data)
set.seed(123)
test_index <- createDataPartition(bank$Personal.Loan, p = 0.2, list = FALSE) # 20%
test_data <- bank[test_index, ]
bank_data <- bank[-test_index, ]
train_index <- createDataPartition(bank_data$Personal.Loan, p = .5, list = FALSE) # 50%
train_data <- bank_data[train_index]
valid_data <- bank_data[-train_index] # 30%
summary(test_data)
summary(train_data)
summary(valid_data)
cat("Train rows:", nrow(train_data), "\n")
cat("Validation rows:", nrow(valid_data), "\n")
cat("Test rows:", nrow(test_data), "\n")
set.seed(123)
test_index <- createDataPartition(bank$Personal.Loan, p = 0.2, list = FALSE) # 20%
test_data <- bank[test_index, ]
bank_data <- bank[-test_index, ]
train_index <- createDataPartition(bank_data$Personal.Loan, p = .5, list = FALSE) # 50%
train_data <- bank_data[train_index, ]
valid_data <- bank_data[-train_index, ] # 30%
summary(test_data)
summary(train_data)
summary(valid_data)
cat("Train rows:", nrow(train_data), "\n")
cat("Validation rows:", nrow(valid_data), "\n")
cat("Test rows:", nrow(test_data), "\n")
set.seed(123)
test_index <- createDataPartition(bank$Personal.Loan, p = 0.2, list = FALSE) # 20%
test_data <- bank[test_index, ]
bank_data <- bank[-test_index, ]
train_index <- createDataPartition(bank_data$Personal.Loan, p = .625, list = FALSE) # 50%
train_data <- bank_data[train_index, ]
valid_data <- bank_data[-train_index, ] # 30%
summary(test_data)
summary(train_data)
summary(valid_data)
cat("Train rows:", nrow(train_data), "\n")
cat("Validation rows:", nrow(valid_data), "\n")
cat("Test rows:", nrow(test_data), "\n")
library("class")
library("FNN")
library("gmodels")
set.seed(123)
test_index <- createDataPartition(bank$Personal.Loan, p = 0.2, list = FALSE) # 20%
test_data <- bank[test_index, ]
bank_data <- bank[-test_index, ]
train_index <- createDataPartition(bank_data$Personal.Loan, p = .625, list = FALSE) # 50%
train_data <- bank_data[train_index, ]
valid_data <- bank_data[-train_index, ] # 30%
summary(test_data)
summary(train_data)
summary(valid_data)
# Training
train_X <- train_data[, !(names(train_data) %in% c("Personal.Loan"))]
train_Y <- factor(train_data$Personal.Loan)
# Validation
valid_X <- valid_data[, !(names(valid_data) %in% c("Personal.Loan"))]
valid_Y <- factor(valid_data$Personal.Loan)
# Test
test_X <- test_data[, !(names(test_data) %in% c("Personal.Loan"))]
test_Y <- factor(test_data$Personal.Loan)
best_k <- 3
# Predictions
train_pred <- knn(train = train_X, test = train_X, cl = train_Y, k = best_k)
valid_pred <- knn(train = train_X, test = valid_X, cl = train_Y, k = best_k)
test_pred <- knn(train = train_X, test = test_X, cl = train_Y, k = best_k)
# Training
CrossTable(x = train_Y, y = train_pred, prop.chisq = FALSE)
# Validation
CrossTable(x = valid_Y, y = valid_pred, prop.chisq = FALSE)
# Test
CrossTable(x = test_Y, y = test_pred, prop.chisq = FALSE)
library("class")
library("FNN")
library("gmodels")
set.seed(123)
test_index <- createDataPartition(bank$Personal.Loan, p = 0.2, list = FALSE) # 20%
test_data <- bank[test_index, ]
bank_data <- bank[-test_index, ]
train_index <- createDataPartition(bank_data$Personal.Loan, p = .625, list = FALSE) # 50%
train_data <- bank_data[train_index, ]
valid_data <- bank_data[-train_index, ] # 30%
summary(test_data)
summary(train_data)
summary(valid_data)
# Training
train_X <- train_data[, !(names(train_data) %in% c("Personal.Loan"))]
train_Y <- factor(train_data$Personal.Loan)
# Validation
valid_X <- valid_data[, !(names(valid_data) %in% c("Personal.Loan"))]
valid_Y <- factor(valid_data$Personal.Loan)
# Test
test_X <- test_data[, !(names(test_data) %in% c("Personal.Loan"))]
test_Y <- factor(test_data$Personal.Loan)
best_k <- 3
# Predictions
train_pred <- knn(train = train_X, test = train_X, cl = train_Y, k = best_k)
valid_pred <- knn(train = train_X, test = valid_X, cl = train_Y, k = best_k)
test_pred <- knn(train = train_X, test = test_X, cl = train_Y, k = best_k)
# Training
CrossTable(x = train_Y, y = train_pred, prop.chisq = FALSE)
# Validation
CrossTable(x = valid_Y, y = valid_pred, prop.chisq = FALSE)
# Test
CrossTable(x = test_Y, y = test_pred, prop.chisq = FALSE)
getwd()
setwd("C:/Users/Cole Yagersz/)
j
getwd()
setwd("C:/Users/Cole Yagersz)
""
efficiency_tabe <- data.frame(
DMU = LETTERS[1:6],
CRS = dea_crs,
VRS = dea_vrs,
IRS = dea_irs,
DRS = dea_drs,
FDH = dea_fdh,
FRH = dea_frh
)
library("lpSolveAPI")   # loads the lp solve library
library("Benchmarking") # loads the library for the DEA Model
x <- matrix(c(150,400, 320, 520, 350, 320,200,700,1200,2000,1200,700),ncol=2) # Convert Supplies because it is in thousands per Day
y <- matrix(c(14000, 14000, 42000, 28000, 19000, 14000,3500, 21000, 10500, 42000, 25000, 15000),ncol=2)
colnames(y) <- c("Reimbursed","Privately Paid")
colnames(x) <- c("Staff Hours per Day","Supplies per Day")
x
y
dea_crs <- dea(x,y,RTS ="crs")
print(dea_crs)
print(peers(dea_crs))
print(lambda(dea_crs))
dea.plot.transform(y[,1], y[,2], RTS = "crs")
dea_vrs <- dea(x,y,RTS ="vrs")
print(dea_vrs)
print(peers(dea_vrs))
print(lambda(dea_vrs))
dea.plot.transform(y[,1], y[,2], RTS = "vrs")
dea_irs <- dea(x,y,RTS ="irs")
print(dea_irs)
print(peers(dea_irs))
print(lambda(dea_irs))
dea.plot.transform(y[,1], y[,2], RTS = "irs")
dea_drs <- dea(x,y,RTS ="drs")
print(dea_drs)
print(peers(dea_drs))
print(lambda(dea_drs))
dea.plot.transform(y[,1], y[,2], RTS = "drs")
dea_fdh <- dea(x,y,RTS ="fdh")
print(dea_fdh)
print(peers(dea_fdh))
print(lambda(dea_fdh))
dea.plot.transform(y[,1], y[,2], RTS = "fdh")
dea_frh <- dea(x,y,RTS = "add")
print(dea_frh)
print(peers(dea_frh))
print(lambda(dea_frh))
dea.plot.transform(y[,1], y[,2], RTS = "add")
dea_frh <- dea(x,y,RTS = "add")
print(dea_frh)
print(peers(dea_frh))
print(lambda(dea_frh))
#No Plot model available for FRH
efficiency_tabe <- data.frame(
DMU = LETTERS[1:6],
CRS = dea_crs,
VRS = dea_vrs,
IRS = dea_irs,
DRS = dea_drs,
FDH = dea_fdh,
FRH = dea_frh
)
efficiency_tabe <- data.frame(
DMU = LETTERS[1:6],
CRS = efficientcies(dea_crs),
VRS = efficientcies(dea_vrs),
IRS = efficientcies(dea_irs),
DRS = efficientcies(dea_drs),
FDH = efficientcies(dea_fdh),
FRH = efficientcies(dea_frh)
)
efficiency_tabe <- data.frame(
DMU = LETTERS[1:6],
CRS = efficiencies(dea_crs),
VRS = efficiencies(dea_vrs),
IRS = efficiencies(dea_irs),
DRS = efficiencies(dea_drs),
FDH = efficiencies(dea_fdh),
FRH = efficiencies(dea_frh)
)
efficiency_table <- data.frame(
DMU = LETTERS[1:6],
CRS = efficiencies(dea_crs),
VRS = efficiencies(dea_vrs),
IRS = efficiencies(dea_irs),
DRS = efficiencies(dea_drs),
FDH = efficiencies(dea_fdh),
FRH = efficiencies(dea_frh)
)
print(efficiency_table)
efficiency_table <- data.frame(
DMU = [1:6],
DMU_eff <- paste("Facilty",1:6)
efficiency_table <- data.frame(
DMU = DMU_eff,
CRS = efficiencies(dea_crs),
VRS = efficiencies(dea_vrs),
IRS = efficiencies(dea_irs),
DRS = efficiencies(dea_drs),
FDH = efficiencies(dea_fdh),
FRH = efficiencies(dea_frh)
)
print(efficiency_table)
dea_crs <- dea(x,y,RTS ="crs")
print(dea_crs)
print(peers(dea_crs))
print(lambda(dea_crs))
dea.plot.transform(y[,1], y[,2], RTS = "crs")
dea_vrs <- dea(x,y,RTS ="vrs")
print(dea_vrs)
print(peers(dea_vrs))
print(lambda(dea_vrs))
dea.plot.transform(y[,1], y[,2], RTS = "vrs")
dea_irs <- dea(x,y,RTS ="irs")
print(dea_irs)
print(peers(dea_irs))
print(lambda(dea_irs))
dea.plot.transform(y[,1], y[,2], RTS = "irs")
dea_drs <- dea(x,y,RTS ="drs")
print(dea_drs)
print(peers(dea_drs))
print(lambda(dea_drs))
dea.plot.transform(y[,1], y[,2], RTS = "drs")
dea_fdh <- dea(x,y,RTS ="fdh")
print(dea_fdh)
print(peers(dea_fdh))
print(lambda(dea_fdh))
dea.plot.transform(y[,1], y[,2], RTS = "fdh")
dea_frh <- dea(x,y,RTS = "add")
print(dea_frh)
print(peers(dea_frh))
print(lambda(dea_frh))
#No Plot model available for FRH
DMU_eff <- paste("Facilty",1:6)
efficiency_table <- data.frame(
DMU = DMU_eff,
CRS = efficiencies(dea_crs),
VRS = efficiencies(dea_vrs),
IRS = efficiencies(dea_irs),
DRS = efficiencies(dea_drs),
FDH = efficiencies(dea_fdh),
FRH = efficiencies(dea_frh)
)
print(efficiency_table)
getwd()
setwd("C:/Users/Cole Yagersz")
x <- matrix(c(150,400, 320, 520, 350, 320,.2,.7,1.2,2,1.2,.7),ncol=2) # Convert Supplies because it is in thousands per Day
y <- matrix(c(14000, 14000, 42000, 28000, 19000, 14000,3500, 21000, 10500, 42000, 25000, 15000),ncol=2)
colnames(y) <- c("Reimbursed","Privately Paid")
colnames(x) <- c("Staff Hours per Day","Supplies per Day")
x
y
x <- matrix(c(150,400, 320, 520, 350, 320,.2,.7,1.2,2,1.2,.7),ncol=2) # Convert Supplies because it is in thousands per Day
y <- matrix(c(14000, 14000, 42000, 28000, 19000, 14000,3500, 21000, 10500, 42000, 25000, 15000),ncol=2)
colnames(y) <- c("Reimbursed","Privately Paid")
colnames(x) <- c("Staff Hours per Day","Supplies per Day")
cat("Input Data:\n")
print(x)
cat("Output Data:\n")
print(y)
cat("CRS:\n")
dea_crs <- dea(x,y,RTS ="crs")
library("lpSolveAPI")   # loads the lp solve library
library("Benchmarking") # loads the library for the DEA Model
cat("CRS:\n")
dea_crs <- dea(x,y,RTS ="crs")
print(dea_crs)
print(peers(dea_crs))
print(lambda(dea_crs))
dea.plot.transform(y[,1], y[,2], RTS = "crs")
cat("CRS:\n")
dea_crs <- dea(x,y,RTS ="crs")
print(dea_crs)
print(peers(dea_crs))
print(lambda(dea_crs))
dea.plot(x, y, RTS = "crs", ORIENTATION = "in-out", txt = rownames(x))
dea_vrs <- dea(x,y,RTS ="vrs")
print(dea_vrs)
print(peers(dea_vrs))
print(lambda(dea_vrs))
dea.plot(x, y, RTS = "vrs", ORIENTATION = "in-out", txt = rownames(x))
dea_irs <- dea(x,y,RTS ="irs")
print(dea_irs)
print(peers(dea_irs))
print(lambda(dea_irs))
dea.plot(x, y, RTS = "irs", ORIENTATION = "in-out", txt = rownames(x))
dea_drs <- dea(x,y,RTS ="drs")
print(dea_drs)
print(peers(dea_drs))
print(lambda(dea_drs))
dea.plot(x, y, RTS = "drs", ORIENTATION = "in-out", txt = rownames(x))
dea_fdh <- dea(x,y,RTS ="fdh")
print(dea_fdh)
print(peers(dea_fdh))
print(lambda(dea_fdh))
dea.plot(x, y, RTS = "fdh", ORIENTATION = "in-out", txt = rownames(x))
dea_frh <- dea(x,y,RTS = "add") # Uses add because FRH is not supported by Benchmarking Library
print(dea_frh)
print(peers(dea_frh))
print(lambda(dea_frh))
dea.plot(x, y, RTS = "add", ORIENTATION = "in-out", txt = rownames(x))
DMU_eff <- paste("Facility",1:6)
efficiency_table <- data.frame(
DMU = DMU_eff,
CRS = efficiencies(dea_crs),
VRS = efficiencies(dea_vrs),
IRS = efficiencies(dea_irs),
DRS = efficiencies(dea_drs),
FDH = efficiencies(dea_fdh),
FRH = efficiencies(dea_frh)
)
print(efficiency_table)
set.seed(123)  # ensures reproducibility
km3 <- kmeans(pharma_scaled, centers = 3, nstart = 25)
install.packages("tidyverse")
install.packages("factoextra")
install.packages("ISLR")
install.packages("tidyverse")
install.packages("factoextra")
library(tidyverse)
library(factoextra)
library(ISLR)
library(flexclust)
# Read the dataset
pharma <- read.csv("Pharmaceuticals.csv")
head(pharma)
# Just use the numerical variables
pharma_num <- pharma[,3:11]
# Standardize the data so all variables have equal weight
pharma_scaled <- scale(pharma_num)
# Justification:
# Scaling makes sure that large-scale variables do not dominate smaller ones
# This gives equal weight to each variable in clustering.
# Elbow method
fviz_nbclust(pharma_scaled, kmeans, method = "wss") +
ggtitle("Elbow Method for Choosing Optimal Number of Clusters")
set.seed(123)  # ensures reproducibility
km3 <- kmeans(pharma_scaled, centers = 3, nstart = 25)
print("The Number of Companies in each Cluster")
print(km3$size)
print("The Number of Centers")
print(km3$centers)
fviz_cluster(km3, data = df) # Visualize the output
set.seed(123)  # ensures reproducibility
km3 <- kmeans(pharma_scaled, centers = 3, nstart = 25)
print("The Number of Companies in each Cluster")
print(km3$size)
print("The Number of Centers")
print(km3$centers)
fviz_cluster(km3, data = pharma_scaled) # Visualize the output
print(km3)
# Elbow method
fviz_nbclust(pharma_scaled, kmeans, method = "wss") +
ggtitle("Elbow Method for Choosing Optimal Number of Clusters")
set.seed(123)  # ensures reproducibility
km3 <- kmeans(pharma_scaled, centers = 3, nstart = 25)
print("The Number of Companies in each Cluster")
print(km3$size)
print("The Number of Centers")
print(km3$centers)
fviz_cluster(km3, data = pharma_scaled) # Visualize the output
print(km3)
set.seed(123)  # ensures reproducibility
km3 <- kmeans(pharma_scaled, centers = 3, nstart = 25)
print("The Number of Companies in each Cluster")
print(km3$size)
print("The Number of Centers")
print(km3$centers)
fviz_cluster(km3, data = pharma_scaled) # Visualize the output
print(km3)
set.seed(123)  # ensures reproducibility
km3 <- kmeans(pharma_scaled, centers = 3, nstart = 25)
print("The Number of Companies in each Cluster")
print(km3$size)
print("The Number of Centers")
print(km3$centers)
fviz_cluster(km3, data = pharma_scaled) # Visualize the output
set.seed(123)  # ensures reproducibility
km3 <- kmeans(pharma_scaled, centers = 3, nstart = 25)
print("The Number of Companies in each Cluster")
print(km3$size)
print("The Output of Centers")
print(km3$centers)
fviz_cluster(km3, data = pharma_scaled) # Visualize the output
# Add cluster assignments
pharma_clustered <- pharma %>%
mutate(Cluster = factor(km3$cluster))
# Summarize median recommendation
aggregate(Median_Recommendation ~ Cluster, data = pharma_clustered, mean)
# Summarize Location of Headquarters
table(pharma_clustered$Cluster, pharma_clustered$Location)
# Summarize Stock Exchange
table(pharma_clustered$Cluster, pharma_clustered$Stock_Exchange)
library(tidyverse)
library(factoextra)
library(ISLR)
library(flexclust)
# Read the dataset
pharma <- read.csv("Pharmaceuticals.csv")
head(pharma)
# Just use the numerical variables
pharma_num <- pharma[,3:11]
# Standardize the data so all variables have equal weight
pharma_scaled <- scale(pharma_num)
# Justification:
# Scaling makes sure that large-scale variables do not dominate smaller ones
# This gives equal weight to each variable in clustering.
# Add cluster assignments
pharma_clustered <- pharma %>%
mutate(Cluster = factor(km3$cluster))
# Summarize median recommendation
aggregate(Median_Recommendation ~ Cluster, data = pharma_clustered, mean)
# Summarize Location of Headquarters
table(pharma_clustered$Cluster, pharma_clustered$Location)
# Summarize Stock Exchange
table(pharma_clustered$Cluster, pharma_clustered$Exchange)
# Add cluster assignments
pharma_clustered <- pharma %>%
mutate(Cluster = factor(km3$cluster))
# Summarize Location of Headquarters
table(pharma_clustered$Cluster, pharma_clustered$Location)
# Summarize Stock Exchange
table(pharma_clustered$Cluster, pharma_clustered$Exchange)
# Add cluster assignments
pharma_clustered <- pharma %>%
mutate(Cluster = factor(km3$cluster))
# Summarize Location of Headquarters
table(pharma_clustered$Cluster, pharma_clustered$Location)
# Summarize Stock Exchange
table(pharma_clustered$Cluster, pharma_clustered$Exchange)
# Add cluster assignments
pharma_clustered <- pharma %>%
mutate(Cluster = factor(km3$cluster))
table(pharma_clustered$Cluster, pharma_clustered$Median_Recommendation)
# Summarize Location of Headquarters
table(pharma_clustered$Cluster, pharma_clustered$Location)
# Summarize Stock Exchange
table(pharma_clustered$Cluster, pharma_clustered$Exchange)
# Add cluster assignments
pharma_clustered <- pharma %>%
mutate(Cluster = factor(km3$cluster))
table(pharma_clustered$Cluster, pharma_clustered$Median_Recommendation)
# Summarize Location of Headquarters
table(pharma_clustered$Cluster, pharma_clustered$Location)
# Summarize Stock Exchange
table(pharma_clustered$Cluster, pharma_clustered$Exchange)
